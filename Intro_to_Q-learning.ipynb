{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60aa6c07-cd6a-4a6e-82cb-696dd32bc7e3",
   "metadata": {},
   "source": [
    "# What is Reinforcement Learning? \n",
    "Reinforcement learning is a framework that is used to solve control tasks. Through RL, agents learn from their environment by interacting within the environment and receiving rewards as feedback. \n",
    "\n",
    "## What is the goal of reinforcement learning? \n",
    "The goal of reinforcement is to maximize the expected cumulative reward in the environment "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7433a952-a272-4e6c-b31e-c6d0e82ce25e",
   "metadata": {},
   "source": [
    "# Two approaches to finding the optimal policy\n",
    "* Policy-based methods\n",
    "    * Policy is trained directly through which the agent learns what actions to take given the current state\n",
    "* Value-based methods:\n",
    "    * Train a value function to learn which state is more valuable and our agent takes actions to reach this state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af60af22-0274-4e3f-9573-c2e7192b2b74",
   "metadata": {},
   "source": [
    "## Value-based methods\n",
    "\n",
    "Value of a state s is the expected discounted return the agent can get if it starts at state s and acts according to policy. \n",
    "\n",
    "### What is the policy in value-based methods? \n",
    "In value-based methods, our policy takes action based on the value function. The policy is not trained explicitly in value-based methods, as such the behavior of the model must be defined by hand. The simplest policy in value-based methods is Greedy Policy where the we choose the action that has the highest value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd03403-8a4a-46e0-9438-7b179410288e",
   "metadata": {},
   "source": [
    "### Two types of value-based methods\n",
    "* State-value function\n",
    "    * For each state s, the function returns the expected return V if the agent starts at s and follows the policy till termination\n",
    "* Action-value function\n",
    "    * Given a state s and action a, the function returns the expected return Q if the agent starts at s and takes action a and follows the policy till termination\n",
    "* In both cases, we need to sum all the rewards an agent can get if it starts at state s   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127e8139-121f-4228-898c-cd9c23545893",
   "metadata": {},
   "source": [
    "### The Bellman equation \n",
    "To calculate the $V(S_{t})$ we need to calculate the expected return starting at state $S_t$ till termination. To calculate $V(S_{t+1})$ we need to calculate the expected return starting at state $S_{t+1}$ till termination. As we can see there is a lot of redundant calculations. The Bellman equation is a recursive function that can significantly reduce these redundant calculations. \n",
    "\n",
    "Under the Bellman equation, the value of state the immediate reward $R_{t+1}$+ the discount factor gamma times the value of the subsequent state $(gamma * V(S_{t+1}))$\n",
    "\n",
    "**Bellman Equation**\n",
    "\n",
    "$V(S_t) = R_{t+1} + gamma * V(S_{t+1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa4bf54-1627-4067-a0f1-ad5989c35d48",
   "metadata": {},
   "source": [
    "## Monte Carlo vs Temporal Difference Learning\n",
    "\n",
    "Simply put, the Monte Carlo strategy uses an entire episode of experience before learning, while the Temporal difference strategy uses just a single step to start learning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6c5a6f-11ae-4c72-bc51-6905ea2bb325",
   "metadata": {},
   "source": [
    "### Monte Carlo \n",
    "When using monte carlo we wait for an entire episode to complete before we calculate $G_t$ (return) and use it as a target to update the value function. \n",
    "\n",
    "$V(S_t) <-- V(S_t) + lr[G_t - V(S_t)]$\n",
    "\n",
    "* Each episode starts at the same position\n",
    "* The agent takes actions according to the policy\n",
    "* We store the state, actions, rewards and next states in a tuple\n",
    "* We repeat this process till the episode terminates\n",
    "* We sum up all the rewards the agent receives $G_t$\n",
    "* We then update $V(S_t)$ based on the total rewards $G_t$ using the above formula  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f62844e-6369-4ac6-910a-fa1063f6975c",
   "metadata": {},
   "source": [
    "### Temporal Difference Learning \n",
    "When using Temporal Difference Learning, we use a single step to update $V(S_t)$. \n",
    "\n",
    "We update $V(S_t)$ at each step. Since we didn't complete an entire episode, we don't have $G_t$, instead, we estimate $G_t$ by adding the immediate reward $R_{t+1}$ and the discounted value of the next state. \n",
    "\n",
    "$V(S_t)<-- V(S_t)+ lr[R_{t+1} + gamma(V(S_{t+1})) - V(S_t)]$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eee03c5-01ec-46c3-a5ca-2937360fb37d",
   "metadata": {},
   "source": [
    "## What is Q-Learning? \n",
    "\n",
    "Q-Learning is a off-policy value-based method that uses Temporal Difference(TD) approach to train an action-value function. \n",
    "\n",
    "In Q-learning, we train a Q-function, an action-value function which outputs the value of taking an action in a given state. \n",
    "\n",
    "The Q-function is encoded into a Q-table, which is a table that stores state-action pair values. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9776479d-a1b8-4feb-8694-4ade97088658",
   "metadata": {},
   "source": [
    "### Q-learning algorithm \n",
    "* Step 1: Initialize the Q-table for each state and action to zero\n",
    "* Step 2: Choose an action using the epsilon-greedy strat\n",
    "    * Initialize epsilon with 1.0\n",
    "    * with probability 1-epsilon we take actions prescribed by our q-table\n",
    "    * with probability epsilon we choose a random action\n",
    "    * Since epsilon is 1.0 at the beginning, we choose a random action, as time progresses and we fill our q-table, we lower our epsilon value.\n",
    "* Step 3: Perform action $A_t$, get reward $R_{t+1}$ and next state $S_{t+1}$\n",
    "* Step 4: Update $Q(S_t,A_t)$\n",
    "    * $Q(S_t, A_t) <--Q(S_t, A_t) + lr[R_{t+1} + gamma*max_aQ(S_{t+1},a) - Q(S_t,A_t)]$\n",
    "    * We update $Q(S_t,A_t)$ using the formula above\n",
    "    1. Obtain reward $R_{t+1}$ after taking action $A_t$\n",
    "    2. Get the best-action pair value by using the greedy policy to select the best action which will be the action with the highest state-action value.\n",
    "* repeat the process.     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32013e6f-34db-4bf2-9ac2-ccf760c9c7e0",
   "metadata": {},
   "source": [
    "#### Off-policy vs On-policy \n",
    "* Off-policy: using a different policy for acting(inference) and updating(training)\n",
    "    * In the case of Q-learning, epsilon-greedy is used for acting, while the greedy policy is used to update our Q-value function.\n",
    "* On-policy: using the same policy for acting and updating\n",
    "    * In the case of Sarsa,  the epsilon-greedy policy is used to select the next-state action pair instead of greedy policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec4d827-48c1-4e5f-9e00-bd2fb62fea2f",
   "metadata": {},
   "source": [
    "## Q-learning example Frozen Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e53e60c-75af-4e0b-95fa-2dc926109d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____OBSERVATION SPACE_____ \n",
      "\n",
      "Observation Space Discrete(16)\n",
      "Sample Observation 4\n",
      "\n",
      " _____Action Space_____ \n",
      "\n",
      "Action Space Shape 4\n",
      "Action Space sample 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import gymnasium as gym\n",
    "import random\n",
    "import os \n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", map_name = \"4x4\", is_slippery=False,render_mode=\"rgb_array\")\n",
    "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
    "print(\"Observation Space\",env.observation_space)\n",
    "print(\"Sample Observation\", env.observation_space.sample())\n",
    "\n",
    "print(\"\\n _____Action Space_____ \\n\")\n",
    "print(\"Action Space Shape\", env.action_space.n)\n",
    "print(\"Action Space sample\", env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55867bb3-b3a8-4929-9311-6d0d0c6c6e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are  16  possible states\n",
      "There are  4  possible actions\n"
     ]
    }
   ],
   "source": [
    "state_space = env.observation_space.n\n",
    "print(\"There are \", state_space, \" possible states\")\n",
    "action_space = env.action_space.n\n",
    "print(\"There are \", action_space, \" possible actions\")\n",
    "def initialize_q_table(state_space, action_space):\n",
    "    Qtable = np.zeros((state_space,action_space))\n",
    "    return Qtable\n",
    "\n",
    "Qtable_frozenlake = initialize_q_table(state_space, action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19a963c5-88d7-4b13-b1f8-02d5b55ced7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_policy(Qtable,state):\n",
    "    action = np.argmax(Qtable[state][:])\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bec3f02-c739-4688-ba55-fcb58686ffad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(Qtable, state, epsilon):\n",
    "    random_num = random.uniform(0,1)\n",
    "\n",
    "    if random_num > epsilon:\n",
    "        action = greedy_policy(Qtable,state)\n",
    "\n",
    "    else:\n",
    "        action = env.action_space.sample()\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe67f3ca-0128-4d7b-b550-99a57b728223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters \n",
    "n_training_episodes = 10000 # total training episodes \n",
    "learning_rate = 0.7\n",
    "\n",
    "# Evaluation parameters \n",
    "n_eval_episodes = 100\n",
    "\n",
    "# Env parameters \n",
    "env_id = 'FrozenLake-v1'\n",
    "max_steps = 99\n",
    "gamma = 0.95\n",
    "eval_seed = []\n",
    "\n",
    "# Exploration params \n",
    "max_epsilon = 1.0 \n",
    "min_epsilon = 0.05\n",
    "decay_rate = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce3c7e78-0c76-4a34-8f50-f3b575648b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable):\n",
    "    for episode in tqdm(range(n_training_episodes)):\n",
    "        epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
    "        state, info = env.reset()\n",
    "        step = 0 \n",
    "        terminated = False\n",
    "        truncated = False\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            action = epsilon_greedy_policy(Qtable, state, epsilon)\n",
    "\n",
    "            new_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "            Qtable[state][action] = Qtable[state][action] + learning_rate * (\n",
    "                reward+ gamma*np.max(Qtable[new_state])-Qtable[state][action])\n",
    "\n",
    "            if terminated or truncated: \n",
    "                break\n",
    "\n",
    "            state= new_state\n",
    "\n",
    "    return Qtable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc3e7294-d9f0-47c0-8cb0-897d3b3f2477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a5b8eeba15e4496af85efc0af56ae88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Qtable_frozenlake = train(n_training_episodes,min_epsilon,max_epsilon,decay_rate,env,max_steps,Qtable_frozenlake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ecc0ee04-538b-48c7-b19d-7c7399e58e8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.73509189, 0.77378094, 0.77378094, 0.73509189],\n",
       "       [0.73509189, 0.        , 0.81450625, 0.77378094],\n",
       "       [0.77378094, 0.857375  , 0.77378094, 0.81450625],\n",
       "       [0.81450625, 0.        , 0.77378094, 0.77378094],\n",
       "       [0.77378094, 0.81450625, 0.        , 0.73509189],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.9025    , 0.        , 0.81450625],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.81450625, 0.        , 0.857375  , 0.77378094],\n",
       "       [0.81450625, 0.9025    , 0.9025    , 0.        ],\n",
       "       [0.857375  , 0.95      , 0.        , 0.857375  ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.9025    , 0.95      , 0.857375  ],\n",
       "       [0.9025    , 0.95      , 1.        , 0.9025    ],\n",
       "       [0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Qtable_frozenlake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "203e2682-5c98-4979-93e5-5e1497b6216e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(env, max_steps, n_eval_episodesm,Q, seed):\n",
    "    episode_rewards = []\n",
    "    for episode in tqdm(range(n_eval_episodes)):\n",
    "        if seed:\n",
    "            state, info = env.reset(seed =seed[episode])\n",
    "        else:\n",
    "            state, info = env.reset()\n",
    "\n",
    "        step = 0\n",
    "        truncated = False\n",
    "        terminated = False\n",
    "        total_rewards_ep = 0\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            action = greedy_policy(Q, state)\n",
    "            new_state, reward, terminated, truncated, info = env.step(action)\n",
    "            total_rewards_ep+=reward\n",
    "\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "            state = new_state\n",
    "        episode_rewards.append(total_rewards_ep)\n",
    "    mean_reward = np.mean(episode_rewards)\n",
    "    std_reward = np.std(episode_rewards)\n",
    "\n",
    "    return mean_reward, std_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5cf45b8-10e1-4f5c-8567-7eec2e8f15ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3280a2e0350452fa8d9f1621c37ea5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean_reward=1.00 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "mean_reward, std_reward = evaluate_agent(env, max_steps, n_eval_episodes, Qtable_frozenlake, eval_seed)\n",
    "print(f\"Mean_reward={mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ffe827-2541-4d00-91da-65a4b10e7fae",
   "metadata": {},
   "source": [
    "## Model taking actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6477aeee-39f6-4a92-a32c-2a4adc635f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:1.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v1',map_name = \"4x4\", is_slippery=False,render_mode='human')\n",
    "observation,info=env.reset()\n",
    "done=False\n",
    "score=0\n",
    "steps=0\n",
    "while not done: \n",
    "    action=greedy_policy(Qtable_frozenlake,observation)\n",
    "    observation,reward,done,truncated,info=env.step(action)\n",
    "    score+=reward\n",
    "    steps+=1\n",
    "    env.render()\n",
    "print(f'Score:{score}')\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb60f836-f567-4b29-91b1-d56b3cc7ec78",
   "metadata": {},
   "source": [
    "## Q-Learning example #2: Taxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa6e3a7c-a8c4-4aa1-9891-1d0515e86516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are  500  possible states\n",
      "There are  6  possible actions\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Taxi-v3\", render_mode = \"rgb_array\")\n",
    "state_space = env.observation_space.n\n",
    "print(\"There are \", state_space, \" possible states\")\n",
    "\n",
    "action_space = env.action_space.n\n",
    "print(\"There are \", action_space, \" possible actions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74df8663-cf40-4e6a-af75-ba5a4058b5d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n",
      "Q-table shape:  (500, 6)\n"
     ]
    }
   ],
   "source": [
    "Qtable_taxi = initialize_q_table(state_space, action_space)\n",
    "print(Qtable_taxi)\n",
    "print(\"Q-table shape: \", Qtable_taxi.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec01348c-1606-402e-8b01-d291282d9060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters \n",
    "n_training_episodes = 25000\n",
    "learning_rate = 0.7\n",
    "\n",
    "# Evaluation params \n",
    "n_eval_episodes = 100\n",
    "\n",
    "eval_seed = [\n",
    "     16,\n",
    "    54,\n",
    "    165,\n",
    "    177,\n",
    "    191,\n",
    "    191,\n",
    "    120,\n",
    "    80,\n",
    "    149,\n",
    "    178,\n",
    "    48,\n",
    "    38,\n",
    "    6,\n",
    "    125,\n",
    "    174,\n",
    "    73,\n",
    "    50,\n",
    "    172,\n",
    "    100,\n",
    "    148,\n",
    "    146,\n",
    "    6,\n",
    "    25,\n",
    "    40,\n",
    "    68,\n",
    "    148,\n",
    "    49,\n",
    "    167,\n",
    "    9,\n",
    "    97,\n",
    "    164,\n",
    "    176,\n",
    "    61,\n",
    "    7,\n",
    "    54,\n",
    "    55,\n",
    "    161,\n",
    "    131,\n",
    "    184,\n",
    "    51,\n",
    "    170,\n",
    "    12,\n",
    "    120,\n",
    "    113,\n",
    "    95,\n",
    "    126,\n",
    "    51,\n",
    "    98,\n",
    "    36,\n",
    "    135,\n",
    "    54,\n",
    "    82,\n",
    "    45,\n",
    "    95,\n",
    "    89,\n",
    "    59,\n",
    "    95,\n",
    "    124,\n",
    "    9,\n",
    "    113,\n",
    "    58,\n",
    "    85,\n",
    "    51,\n",
    "    134,\n",
    "    121,\n",
    "    169,\n",
    "    105,\n",
    "    21,\n",
    "    30,\n",
    "    11,\n",
    "    50,\n",
    "    65,\n",
    "    12,\n",
    "    43,\n",
    "    82,\n",
    "    145,\n",
    "    152,\n",
    "    97,\n",
    "    106,\n",
    "    55,\n",
    "    31,\n",
    "    85,\n",
    "    38,\n",
    "    112,\n",
    "    102,\n",
    "    168,\n",
    "    123,\n",
    "    97,\n",
    "    21,\n",
    "    83,\n",
    "    158,\n",
    "    26,\n",
    "    80,\n",
    "    63,\n",
    "    5,\n",
    "    81,\n",
    "    32,\n",
    "    11,\n",
    "    28,\n",
    "    148,\n",
    "]\n",
    "\n",
    "# Env params \n",
    "env_id = \"Taxi-v3\"\n",
    "max_steps = 99\n",
    "gamma = 0.95\n",
    "\n",
    "#Exploration params \n",
    "max_epsilon = 1.0 \n",
    "min_epsilon = 0.05\n",
    "decay_rate = 0.005\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c89b24bb-6ace-4bee-adfb-22029ce257de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab1045e054b94a4b8157757d6242d309",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[  0.        ,   0.        ,   0.        ,   0.        ,\n",
       "          0.        ,   0.        ],\n",
       "       [  2.75200369,   3.94947756,   2.75200354,   3.94947757,\n",
       "          5.20997639,  -5.05052243],\n",
       "       [  7.93349184,   9.40367562,   5.20997639,   9.40367562,\n",
       "         10.9512375 ,   0.40367562],\n",
       "       ...,\n",
       "       [ -2.65639999,   9.40367562,  -2.56532871,  -2.98871184,\n",
       "         -9.70515   ,  -9.70515   ],\n",
       "       [  1.95364336,   6.53681725,  -5.73854437,  -5.44492963,\n",
       "        -13.59739954, -13.87662576],\n",
       "       [ -1.3755    ,   9.59385   ,   7.12669996,  18.        ,\n",
       "         -7.        ,  -7.        ]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Qtable_taxi = train(n_training_episodes,min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable_taxi)\n",
    "Qtable_taxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68956152-c54b-4498-8164-92d8824f87fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdc4ce6e5bc24015ae8f5c7ecdf81b09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean_reward=7.40 +/- 2.79\n"
     ]
    }
   ],
   "source": [
    "mean_reward, std_reward = evaluate_agent(env, max_steps, n_eval_episodes, Qtable_taxi, eval_seed)\n",
    "print(f\"Mean_reward={mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "085b1170-ef73-4dd3-b064-0dc3e02fb042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:6\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Taxi-v3',render_mode='human')\n",
    "observation,info=env.reset()\n",
    "done=False\n",
    "score=0\n",
    "steps=0\n",
    "while not done: \n",
    "    action=greedy_policy(Qtable_taxi,observation)\n",
    "    observation,reward,done,truncated,info=env.step(action)\n",
    "    score+=reward\n",
    "    steps+=1\n",
    "    env.render()\n",
    "print(f'Score:{score}')\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51051169-5493-431f-bb18-4685c3d710a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
